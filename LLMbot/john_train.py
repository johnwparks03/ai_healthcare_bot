# -*- coding: utf-8 -*-
"""medalpaca_training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n507zW8QwTcoeoFOHYqoEVfO_LQrgZiV
"""
import psycopg2
from datasets import Dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from trl import SFTTrainer


def load_data(limit=None):
    try:
        conn = psycopg2.connect(
            host="database-1.chsyuesiimuq.us-east-2.rds.amazonaws.com",
            database="postgres",
            user="postgres",
            password="abc12345",
            port="5432"
        )

        cur = conn.cursor()

        if limit:
            cur.execute('SELECT * FROM public."QuestionAnswer" ORDER BY RANDOM() LIMIT %s', (limit,))
        else:
            cur.execute('SELECT * FROM public."QuestionAnswer"')
        QA = cur.fetchall()


    except psycopg2.Error as e:
        print(f"Error connecting to PostgreSQL: {e}")

    finally:
        if cur is not None:
            cur.close()
        if conn is not None:
            conn.close()

    return QA

def format_row(example):
    example["text"] = f"""### Instruction:
{example['question']}

### Response:
{example['answer']}"""
    return example

def tokenize(batch):
    return tokenizer(
        batch["text"],
        padding="max_length",
        truncation=True,
        max_length=512,
    )

raw_data = load_data(limit=100)

questions = []
answers = []

for q, a in raw_data:
    questions.append(q)
    answers.append(a)

hf_dataset = Dataset.from_dict({
    "question": questions,
    "answer": answers
})

hf_dataset = hf_dataset.map(format_row)

model_name = "medalpaca/medalpaca-7b"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    load_in_8bit=True   # saves VRAM
)

tokenized = hf_dataset.map(tokenize, batched=True)
tokenized.set_format(type="torch", columns=["input_ids", "attention_mask"])


training_args = TrainingArguments(
    output_dir="medalpaca-trained",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    learning_rate=2e-5,
    fp16=True,
    save_strategy="epoch",
    logging_steps=20,
)

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=tokenized,
    dataset_text_field="text",
    args=training_args,
)

trainer.train()

trainer.model.save_pretrained("medalpaca-disease-classification")
tokenizer.save_pretrained("medalpaca-disease-classification")